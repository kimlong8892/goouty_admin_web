name: Deploy Admin Prod Cloud Run

on:
  push:
    branches: [ "prod" ]
  workflow_dispatch:

env:
  PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
  REGION: asia-southeast1
  REPO_NAME: goouty-repo
  SERVICE_NAME: goouty-admin-prod
  WORKDIR: .

jobs:
  deploy:
    runs-on: ubuntu-latest
    environment: PROD
    permissions:
      contents: 'read'
      id-token: 'write'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      # Setup for Docker Build
      - name: Authenticate to Google Cloud
        id: auth
        uses: google-github-actions/auth@v2
        with:
          credentials_json: '${{ secrets.GCP_SA_KEY }}'

      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2

      - name: Configure Docker to use gcloud as a credential helper
        run: |
          gcloud auth configure-docker ${{ env.REGION }}-docker.pkg.dev

      - name: Build and Push Docker Image
        working-directory: ${{ env.WORKDIR }}
        run: |
          # Build Docker image for prod using the standard Dockerfile
          docker build -f Dockerfile -t ${{ env.REGION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.REPO_NAME }}/${{ env.SERVICE_NAME }}:${{ github.sha }} .
          # Push image to Artifact Registry
          docker push ${{ env.REGION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.REPO_NAME }}/${{ env.SERVICE_NAME }}:${{ github.sha }}

      - name: Prepare Environment Variables
        id: env-vars
        env:
          ALL_SECRETS: ${{ toJson(secrets) }}
          ALL_VARS: ${{ toJson(vars) }}
          # Explicitly reference secrets used in migration to ensure they are available in context
          DB_URL: ${{ secrets.DATABASE_URL }}
          DB_PASS: ${{ secrets.DB_PASSWORD }}
          DB_USER: ${{ secrets.DB_USERNAME }}
          DB_NAME: ${{ secrets.DB_DATABASE }}
        run: |
          echo "Generating environment variables..."

          # 1. Load public variables from env.json
          if [ -f .github/env-config/prod/env.json ]; then
            # Extract basic vars
            jq -r 'to_entries | .[] | "\(.key)=\(.value)"' .github/env-config/prod/env.json > .env.temp
          else
            echo "PORT=8000" > .env.temp
          fi

          # 2. Load secrets based on secrets.json mapping
          if [ -f .github/env-config/prod/secrets.json ]; then
            jq -n --argjson secrets "$ALL_SECRETS" --argjson vars "$ALL_VARS" --argjson mapping "$(cat .github/env-config/prod/secrets.json)" '
              $mapping | to_entries | .[] |
              .key as $k | .value as $v |
              ($secrets[$v] // $vars[$v]) as $val |
              if $val != null then "\($k)=\($val)" else empty end
            ' -r > .env.secrets
            
            if [ -f .env.secrets ] && [ -s .env.secrets ]; then
              cat .env.secrets >> .env.temp
            fi
          fi
          
          # 3. Construct DATABASE_URL if not present but components are
          if ! grep -q "DATABASE_URL=" .env.temp; then
            echo "Attempting to construct DATABASE_URL from components..."
            DB_HOST=$(grep "^DB_HOST=" .env.temp | cut -d= -f2- || echo "")
            DB_PORT=$(grep "^DB_PORT=" .env.temp | cut -d= -f2- || echo "5432")
            DB_NAME=$(grep "^DB_DATABASE=" .env.temp | cut -d= -f2- || grep "^DB_SCHEMA=" .env.temp | cut -d= -f2- || echo "")
            DB_USER=$(grep "^DB_USERNAME=" .env.temp | cut -d= -f2- || echo "")
            DB_PASS=$(grep "^DB_PASSWORD=" .env.temp | cut -d= -f2- || echo "")
            
            if [ -n "$DB_HOST" ] && [ -n "$DB_USER" ] && [ -n "$DB_PASS" ] && [ -n "$DB_NAME" ]; then
               echo "Constructing DATABASE_URL..."
               echo "DATABASE_URL=postgresql://${DB_USER}:${DB_PASS}@${DB_HOST}:${DB_PORT}/${DB_NAME}" >> .env.temp
            fi
          fi

          # 4. Save to GITHUB_OUTPUT for next step
          {
            echo "ENV_VARS<<EOF"
            cat .env.temp
            echo "EOF"
          } >> $GITHUB_OUTPUT
          
          echo "Generated environment variables (keys only):"
          cut -d= -f1 .env.temp
          
          # Extract PORT for Cloud Run flag
          PORT_VAL=$(grep "^PORT=" .env.temp | cut -d= -f2-)
          if [ -z "$PORT_VAL" ]; then
            PORT_VAL="8000"
          fi
          echo "PORT=$PORT_VAL" >> $GITHUB_OUTPUT

      - name: Run Database Migration
        continue-on-error: false
        run: |
          echo "Preparing environment variables for Cloud Run Job..."

          if [ ! -s .env.temp ]; then
            echo "::warning::.env.temp is empty. Skipping migration."
            exit 0
          fi

          # Convert .env format to JSON format for gcloud using jq
          # Exclude PORT as it is a reserved name for Cloud Run Jobs
          grep -v '^#' .env.temp | grep -v '^$' | grep -v '^PORT=' | jq -R 'split("=") | select(length>=2) | {(.[0]): (.[1:] | join("="))}' | jq -s 'add' > env-vars.json

          JOB_NAME="migrate-${{ env.SERVICE_NAME }}"

          echo "Deploying/Updating Cloud Run Job: $JOB_NAME"

          if gcloud run jobs describe $JOB_NAME --region ${{ env.REGION }} > /dev/null 2>&1; then
             gcloud run jobs update $JOB_NAME \
              --image ${{ env.REGION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.REPO_NAME }}/${{ env.SERVICE_NAME }}:${{ github.sha }} \
              --env-vars-file env-vars.json \
              --command npx \
              --args prisma,migrate,deploy,--schema=./prisma/schema.prisma \
              --region ${{ env.REGION }}
          else
             gcloud run jobs create $JOB_NAME \
              --image ${{ env.REGION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.REPO_NAME }}/${{ env.SERVICE_NAME }}:${{ github.sha }} \
              --env-vars-file env-vars.json \
              --command npx \
              --args prisma,migrate,deploy,--schema=./prisma/schema.prisma \
              --region ${{ env.REGION }}
          fi

          echo "Executing migration..."
          EXECUTION_NAME=$(gcloud run jobs execute $JOB_NAME --region ${{ env.REGION }} --format='value(metadata.name)')
          echo "Execution started: $EXECUTION_NAME"
          
          if ! gcloud run executions wait $EXECUTION_NAME --region ${{ env.REGION }}; then
            echo "Migration failed. Fetching container logs for execution: $EXECUTION_NAME"
            # Get the logs from the execution. We use a filter that targets the execution name specifically.
            gcloud logging read "resource.type=cloud_run_job AND resource.labels.job_name=$JOB_NAME AND labels.\"run.googleapis.com/execution_name\"=$EXECUTION_NAME" --limit=200 --format="value(textPayload,jsonPayload.message)"
            exit 1
          fi

      - name: Deploy to Cloud Run
        uses: google-github-actions/deploy-cloudrun@v2
        with:
          service: ${{ env.SERVICE_NAME }}
          region: ${{ env.REGION }}
          image: ${{ env.REGION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.REPO_NAME }}/${{ env.SERVICE_NAME }}:${{ github.sha }}
          env_vars: ${{ steps.env-vars.outputs.ENV_VARS }}
          flags: '--port ${{ steps.env-vars.outputs.PORT }} --allow-unauthenticated'

      - name: Show Cloud Run Service Logs on Failure
        if: failure()
        run: |
          echo "Deployment failed. Fetching latest service logs..."
          gcloud logging read "resource.type=cloud_run_revision AND resource.labels.service_name=${{ env.SERVICE_NAME }} AND resource.labels.location=${{ env.REGION }}" --limit=100
